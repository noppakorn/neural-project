{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffy Fondue Problem Report Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas torch torchtext torchmetrics pythainlp pytorch_lightning wandb scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_NOTEBOOK_NAME=\"traffy-fondue-classification\"\n"
     ]
    }
   ],
   "source": [
    "%env WANDB_NOTEBOOK_NAME=\"traffy-fondue-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m WANDB_NOTEBOOK_NAME should be a path to a notebook file, couldn't find \"traffy-fondue-classification\".\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnoppakorn\u001b[0m (\u001b[33mmeen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logger\n",
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "import torchmetrics\n",
    "\n",
    "from pythainlp.tokenize import word_tokenize\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "from torch import optim, nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "seed_everything(42, workers=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ถนน</td>\n",
       "      <td>[มี, น้ำซึม, ออกมา, บริเวณ, ถนน,  , ทำให้, มี,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ท่อระบายน้ำ</td>\n",
       "      <td>[ท่อน้ำ, แตก,  , น้ำ, ผุด, ไหล, ตลอด]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ท่อระบายน้ำ</td>\n",
       "      <td>[ฝ่า, ท่อ, ชำรุด]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ท่อระบายน้ำ</td>\n",
       "      <td>[ฝา, ท่อ, ชำรุด]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>จราจร</td>\n",
       "      <td>[ตำรวจ, ไม่, จับ, มอ, ไซ, ย้อนศร,  , วิน, มอ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67177</th>\n",
       "      <td>ความสะอาด</td>\n",
       "      <td>[ช่วย, เก็บ, ขยะ, หน่อย, ครับ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67178</th>\n",
       "      <td>ถนน</td>\n",
       "      <td>[ศูนย์, เรื่องราว, ร้องทุกข์,  , ได้รับ, การ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67179</th>\n",
       "      <td>ทางเท้า</td>\n",
       "      <td>[มี, การ, ขาย, น้ำ, กระท่อม,  , และ, กัญชา, เป...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67180</th>\n",
       "      <td>ถนน</td>\n",
       "      <td>[ปัญหา,  , :,  , :,  , ประชาชน, ต้องการ, ให้, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67181</th>\n",
       "      <td>ความสะอาด</td>\n",
       "      <td>[อยาก, ให้, หน่วยงาน, ที่, ดูแลรักษา, ความสะอา...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67182 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               tag                                               text\n",
       "0              ถนน  [มี, น้ำซึม, ออกมา, บริเวณ, ถนน,  , ทำให้, มี,...\n",
       "1      ท่อระบายน้ำ              [ท่อน้ำ, แตก,  , น้ำ, ผุด, ไหล, ตลอด]\n",
       "2      ท่อระบายน้ำ                                  [ฝ่า, ท่อ, ชำรุด]\n",
       "3      ท่อระบายน้ำ                                   [ฝา, ท่อ, ชำรุด]\n",
       "4            จราจร  [ตำรวจ, ไม่, จับ, มอ, ไซ, ย้อนศร,  , วิน, มอ, ...\n",
       "...            ...                                                ...\n",
       "67177    ความสะอาด                     [ช่วย, เก็บ, ขยะ, หน่อย, ครับ]\n",
       "67178          ถนน  [ศูนย์, เรื่องราว, ร้องทุกข์,  , ได้รับ, การ, ...\n",
       "67179      ทางเท้า  [มี, การ, ขาย, น้ำ, กระท่อม,  , และ, กัญชา, เป...\n",
       "67180          ถนน  [ปัญหา,  , :,  , :,  , ประชาชน, ต้องการ, ให้, ...\n",
       "67181    ความสะอาด  [อยาก, ให้, หน่วยงาน, ที่, ดูแลรักษา, ความสะอา...\n",
       "\n",
       "[67182 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./traffy-data-2023.csv\")\n",
    "df = df.loc[:, [\"type\", \"comment\"]]\n",
    "\n",
    "df = df.rename({\"comment\": \"text\", \"type\": \"tag\"}, axis=1)\n",
    "\n",
    "# Data cleansing\n",
    "\n",
    "# Clean the tag\n",
    "def clean_tag(tag: str) -> str:\n",
    "    tag = tag[1:-1]\n",
    "    tag = tag.split(\",\")\n",
    "\n",
    "    if len(tag) == 0:\n",
    "        return \"Unknown\"\n",
    "\n",
    "    if tag[0] == \"ร้องเรียน\" and len(tag) > 1:\n",
    "        return tag[1]\n",
    "    return tag[0]\n",
    "\n",
    "df.loc[:, \"tag\"] = df.loc[:, \"tag\"].apply(clean_tag)\n",
    "df = df[~(df.loc[:, \"tag\"].isin({\"Unknown\", \"\", \"ร้องเรียน\"}))]\n",
    "\n",
    "# Use only tag with >= 1000 reports\n",
    "tags = df.loc[:, \"tag\"].value_counts()\n",
    "tags = tags[(tags.values >= 1000)]\n",
    "df = df[df.loc[:, \"tag\"].isin(tags.keys())]\n",
    "\n",
    "def clean_text(text):\n",
    "    return word_tokenize(text, engine=\"newmm\")\n",
    "\n",
    "df.loc[:, \"text\"] = df.loc[:, \"text\"].apply(clean_text)\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags Statistics\n",
      "tag\n",
      "ถนน            24471\n",
      "ทางเท้า         7750\n",
      "ความสะอาด       5983\n",
      "แสงสว่าง        5108\n",
      "กีดขวาง         4269\n",
      "ความปลอดภัย     3404\n",
      "ป้าย            2863\n",
      "เสียงรบกวน      2705\n",
      "ท่อระบายน้ำ     2547\n",
      "จราจร           2073\n",
      "ต้นไม้          1933\n",
      "สะพาน           1823\n",
      "สัตว์จรจัด      1201\n",
      "สายไฟ           1052\n",
      "Name: count, dtype: int64\n",
      "---\n",
      "Text Statistics\n",
      "Max words per sentence 2492\n",
      "Average words per sentence 57.73838230478402\n"
     ]
    }
   ],
   "source": [
    "print(\"Tags Statistics\")\n",
    "print(df.tag.value_counts())\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "print(\"Text Statistics\")\n",
    "print(\"Max words per sentence\", df.text.apply(len).max())\n",
    "print(\"Average words per sentence\", df.text.apply(len).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(sentences_tokenized):\n",
    "    for word in sentences_tokenized:\n",
    "        yield word\n",
    "\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(df.text.tolist()), specials=[\"<unk>\", \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_idx = {j:i for i, j in enumerate(tags.keys())}\n",
    "idx_to_tag = {j:i for i, j in tag_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraffyDataset(Dataset):\n",
    "    def __init__(self, sentences_tokenized, tag, vocab, tag_to_idx, pad_length=256):\n",
    "        self.sentences_tokenized = sentences_tokenized\n",
    "        self.tag = tag\n",
    "        self.vocab = vocab\n",
    "        self.tag_to_idx = tag_to_idx\n",
    "\n",
    "        self.pad_length = pad_length\n",
    "        self.padding_index = self.vocab[\"<pad>\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences_tokenized)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.vocab(self.sentences_tokenized[idx])\n",
    "        sentence_encoded = torch.LongTensor(tokens[:self.pad_length])\n",
    "\n",
    "        pad = nn.ConstantPad1d(\n",
    "            (0, self.pad_length - len(sentence_encoded)), self.padding_index)\n",
    "        sentence_encoded = pad(sentence_encoded)\n",
    "\n",
    "        tag_encoded = torch.tensor(tag_to_idx[self.tag[idx]])\n",
    "        return sentence_encoded, tag_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TraffyModel(pl.LightningModule):\n",
    "    def __init__(self, vocab_len, class_count, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_len, self.config[\"embedding_dim\"])\n",
    "\n",
    "        # self.dropout = nn.Dropout(p=self.config[\"dropout_rate\"])\n",
    "        self.lstm = nn.LSTM(\n",
    "            self.config[\"embedding_dim\"], self.config[\"lstm_hidden_size\"], batch_first=True)\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            self.config[\"lstm_hidden_size\"], class_count)\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(\n",
    "            task=\"multiclass\", num_classes=class_count)\n",
    "        self.f1 = torchmetrics.F1Score(\n",
    "            task=\"multiclass\", average=\"macro\", num_classes=class_count)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        tokens, labels = batch\n",
    "        x = self.embedding(tokens)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = torch.flatten(x, start_dim=0, end_dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        loss = self.criterion(x, labels)\n",
    "\n",
    "        self.log(\"loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        tokens, labels = batch\n",
    "        x = self.embedding(tokens)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = torch.flatten(x, start_dim=0, end_dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        val_loss = self.criterion(x, labels)\n",
    "\n",
    "        self.log(\"val_loss\", val_loss)\n",
    "\n",
    "        val_acc = self.accuracy(x, labels)\n",
    "        self.log('val_acc', val_acc, on_epoch=True)\n",
    "        val_f1 = self.f1(x, labels)\n",
    "        self.log('val_f1', val_f1, on_epoch=True)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        x = self.embedding(tokens)\n",
    "        _, (x, _) = self.lstm(x)\n",
    "        x = torch.flatten(x, start_dim=0, end_dim=1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(\n",
    "            self.parameters(), lr=self.config[\"lr\"])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"epochs\": 10,\n",
    "    \"lr\": 0.001,\n",
    "    \"embedding_dim\": 300,\n",
    "    \"lstm_hidden_size\": 256,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(df.loc[:, \"text\"].tolist(), df.loc[:, \"tag\"].tolist(), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TraffyDataset(X_train, y_train, vocab, tag_to_idx)\n",
    "val_dataset = TraffyDataset(X_test, y_test, vocab, tag_to_idx)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config[\"batch_size\"])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TraffyModel(vocab_len=len(vocab), class_count=len(tag_to_idx), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230515_190724-giyuklur</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meen/lightning_logs/runs/giyuklur' target=\"_blank\">eternal-voice-2</a></strong> to <a href='https://wandb.ai/meen/lightning_logs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meen/lightning_logs' target=\"_blank\">https://wandb.ai/meen/lightning_logs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meen/lightning_logs/runs/giyuklur' target=\"_blank\">https://wandb.ai/meen/lightning_logs/runs/giyuklur</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"traffy-fondue-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Normal Training\n",
    "# trainer = Trainer(\n",
    "#     logger=logger, min_epochs=config[\"epochs\"], max_epochs=config[\"epochs\"])\n",
    "\n",
    "# Training using gpu\n",
    "trainer = Trainer(accelerator=\"gpu\", devices=1, deterministic=True, logger=wandb_logger,\n",
    "                  min_epochs=config[\"epochs\"], max_epochs=config[\"epochs\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params\n",
      "--------------------------------------------------\n",
      "0 | embedding  | Embedding          | 9.6 M \n",
      "1 | lstm       | LSTM               | 571 K \n",
      "2 | classifier | Linear             | 3.6 K \n",
      "3 | criterion  | CrossEntropyLoss   | 0     \n",
      "4 | accuracy   | MulticlassAccuracy | 0     \n",
      "5 | f1         | MulticlassF1Score  | 0     \n",
      "--------------------------------------------------\n",
      "10.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "10.2 M    Total params\n",
      "40.665    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  5.92it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noppakorn/.virtualenvs/neural-project/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noppakorn/.virtualenvs/neural-project/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 420/420 [01:03<00:00,  6.59it/s, v_num=klur]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 420/420 [01:04<00:00,  6.54it/s, v_num=klur]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model=model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running the model on the validation set after the last epoch\n",
      "Validation DataLoader 0: 100%|██████████| 105/105 [00:06<00:00, 17.14it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Runningstage.validating metric      DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "         val_acc            0.7844012975692749\n",
      "         val_f1             0.7100043892860413\n",
      "        val_loss            0.6673240065574646\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Accuracy on validation set: 0.7844\n",
      "Accuracy on validation set: 0.7100\n"
     ]
    }
   ],
   "source": [
    "print(\"Running the model on the validation set after the last epoch\")\n",
    "res = trainer.validate(model, dataloaders=val_dataloader)\n",
    "print(f\"Accuracy on validation set: {res[0]['val_acc']:.04f}\")\n",
    "print(f\"Accuracy on validation set: {res[0]['val_f1']:.04f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-course",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
